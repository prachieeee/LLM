# LLM
# 🧠 Large Language Model (LLM) from Scratch

This project is an end-to-end implementation of a 124M parameter GPT-2 style Large Language Model built from scratch using PyTorch. The goal is to deeply understand and recreate the core components that power modern LLMs such as GPT-2, while experimenting with training, optimization, and fine-tuning strategies.

## 🚀 Features Implemented

- ✅ Custom Tokenizer  
- ✅ Byte Pair Encoding (BPE)  
- ✅ Positional Encoding  
- ✅ Multi-Head Self-Attention  
- ✅ Transformer Block (In Progress)  
- ✅ GELU Activation (In Progress)  

## 🔧 Tools & Frameworks

- Python  
- PyTorch  
- OpenAI GPT-2 (Reference Architecture)  
- Ollama  
- OpenAI API  

## 🛠️ Roadmap

- [x] Implement Tokenizer  
- [x] Byte Pair Encoding  
- [x] Positional Encoding  
- [x] Multi-Head Attention  
- [ ] GELU Activation  
- [ ] Full Transformer Block  
- [ ] Model Training Pipeline  
- [ ] Evaluation & Text Generation  
- [ ] Instruction-based Fine-Tuning using Alpaca Datasets  
- [ ] Spam Classifier using Fine-Tuned Model  
- [ ] Ollama Integration  
- [ ] Sampling Techniques (Top-k, Temperature Scaling)

## 🧪 Experiments & Fine-Tuning

Planned fine-tuning strategies:
- Alpaca-style Instruction Tuning  
- Task-specific adaptation (e.g., spam detection)  
- Prompt engineering with Ollama and OpenAI APIs  
