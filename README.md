# LLM
# ğŸ§  Large Language Model (LLM) from Scratch

This project is an end-to-end implementation of a 124M parameter GPT-2 style Large Language Model built from scratch using PyTorch. The goal is to deeply understand and recreate the core components that power modern LLMs such as GPT-2, while experimenting with training, optimization, and fine-tuning strategies.

## ğŸš€ Features Implemented

- âœ… Custom Tokenizer  
- âœ… Byte Pair Encoding (BPE)  
- âœ… Positional Encoding  
- âœ… Multi-Head Self-Attention  
- âœ… Transformer Block (In Progress)  
- âœ… GELU Activation (In Progress)  

## ğŸ”§ Tools & Frameworks

- Python  
- PyTorch  
- OpenAI GPT-2 (Reference Architecture)  
- Ollama  
- OpenAI API  

## ğŸ› ï¸ Roadmap

- [x] Implement Tokenizer  
- [x] Byte Pair Encoding  
- [x] Positional Encoding  
- [x] Multi-Head Attention  
- [ ] GELU Activation  
- [ ] Full Transformer Block  
- [ ] Model Training Pipeline  
- [ ] Evaluation & Text Generation  
- [ ] Instruction-based Fine-Tuning using Alpaca Datasets  
- [ ] Spam Classifier using Fine-Tuned Model  
- [ ] Ollama Integration  
- [ ] Sampling Techniques (Top-k, Temperature Scaling)

## ğŸ§ª Experiments & Fine-Tuning

Planned fine-tuning strategies:
- Alpaca-style Instruction Tuning  
- Task-specific adaptation (e.g., spam detection)  
- Prompt engineering with Ollama and OpenAI APIs  
